---
title: Preparing for production
---
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

Deploying WebRTC servers can be tricky, especially in cloud environments. This guide will help you get a secure LiveKit deployment up and running.

## Domain, SSL certificates, and load balancer

In order to have a secure LiveKit desployment, you will need a domain as well as a SSL certificate for that domain. This domain will be used as the primary endpoint for LiveKit clients, for example: `wss://livekit.yourhost.com`. The SSL certificate must be signed by a trusted certificate authority; self-signed certs do not work here.

You will also need to set up HTTPS/SSL termination with a load balancer or reverse proxy.

If you are using TURN (see below), then a separate TURN domain and SSL cert will be needed, as well. In order to use TURN/TLS correctly, the server needs to listen on port 443.

## Ports

LiveKit uses several ports to communicate with clients:

### HTTP/WebSocket

7880 by default. This port should be placed behind a load balancer that can terminate SSL. LiveKit services are homogenous: any client could connect to any backend instance, regardless of the room they are in.

### rtc.port_range_start - rtc.port_range_end

50000-60000 by default. These are the primary ports for UDP data with WebRTC clients. LiveKit advertises these ports as WebRTC host candidates (each participant in the room will use two ports). This port range needs to be open in the firewall, and directly accessible from the Internet.

### rtc.tcp_port

7881 by default. This is used to support ICE over TCP, which is used when the current network configuration does not support UDP (e.g. VPN, corporate firewalls). `rtc.tcp_port` needs to be open in the firewall, and directly accessible from the Internet (i.e. not behind a load balancer).

### turn.tls_port

optional; used by the embedded TURN server (see below)

## TURN

Certain corporate firewalls block not only UDP traffic, but non-secure TCP traffic, as well. In those cases, it's helpful to use a TURN server. [Here's](https://bloggeek.me/webrtc-turn/) a good resource if you're interested in reading more about how TURN is used.

The good news is LiveKit includes an embedded TURN server. It's a secure TURN implementation that has integrated authentication with the rest of LiveKit. The authentication layer ensures that only clients that have already established a signal connection could connect to our TURN server.

### TURN/TLS

To firewalls, TLS traffic looks no different from regular HTTPS traffic to websites. Enabling TURN/TLS gives you the broadest coverage in client connectivity, including those behind corporate firewalls. TURN/TLS can be enabled with:

```yaml title="config.yaml"
turn:
  enabled: true
  tls_port: 5349
  domain: turn.myhost.com
  cert_file: /path/to/turn.crt
  key_file: /path/to/turn.key
```

LiveKit will perform TLS termination, so you will have to specify the certificates in the config. When running multiple LiveKit instances, you can place a layer 4 load balancer in front of the TCP port.

If you are not using a load balancer, `turn.tls_port` needs to be set to 443, as that will be the port that's advertised to clients.

### TURN/UDP

As QUIC or HTTP/3 gain adoption, some firewalls started allowing UDP traffic to pass through port 443. In those cases, it helps to use TURN/UDP on port 443. UDP is preferred over TCP for WebRTC traffic, as it has better control over congestion and latency. TURN/UDP can be enabled with:

```yaml title="config.yaml"
turn:
  enabled: true
  udp_port: 443
```

## Resources

The scalability of LiveKit is bound by CPU and bandwidth. We recommend running production setups on 10Gbps ethernet or faster.

When deploying to cloud providers, compute-optimized instance types are the most suitable for LiveKit.

If running in a Dockerized environment, host networking should be used for optimal performance.

## Configuration

For production deploys, we recommend using a config file. The config file can be passed in via `--config` flag, or the body of the YAML can be set with a `LIVEKIT_CONFIG` environment variable.

Below is a recommended config for a production deploy. To view other customization options, see [config-sample.yaml](https://github.com/livekit/livekit-server/blob/master/config-sample.yaml)

```yaml title="config.yaml"
port: 7880
log_level: info
rtc:
  tcp_port: 7881
  port_range_start: 50000
  port_range_end: 60000
  # use_external_ip should be set to true for most cloud environments where
  # the host has a public IP address, but is not exposed to the process.
  # LiveKit will attempt to use STUN to discover the true IP, and advertise
  # that IP with its clients
  use_external_ip: true
redis:
  # redis is recommended for production deploys
  address: my-redis-server.name:6379
keys:
  # key value pairs
  # your_api_key: <api_secret>
# when enabled, LiveKit will expose prometheus metrics on :6789/metrics
#prometheus_port: 6789
turn:
  enabled: true
  # domain must match tls certificate
  domain: <turn.myhost.com>
  # defaults to 3478. If not using a load balancer, must be set to 443.
  tls_port: 3478
```

## Firewall

When hosting in cloud environments, the ports configured above will have to be opened in the firewall.

<Tabs
  defaultValue="aws"
  groupId="cloud"
  values={[
    {label: 'AWS', value: 'aws'},
    {label: 'Google Cloud', value: 'gcloud'},
  ]}>
  <TabItem value="aws">

Navigate to the VPC dashboard, choose `Security Groups`, and select the security group that LiveKit is deployed to.
Open the `Inbound rules` tab and select `Edit Inbound Rules`

![AWS inbound rules](/img/deploy/aws-inbound-rules.png)

Then add the following rules (assuming use of default ports):

![AWS add rules](/img/deploy/aws-inbound-rules-2.png)

  </TabItem>
  <TabItem value="gcloud">

Navigate to VPC network, then select `Firewall` on the left. Then select `Create Firewall Rule` in the top menu.

The firewall rule should look something like this:

![Google Cloud firewall rules](/img/deploy/gcloud-firewall-rules.png)

  </TabItem>
</Tabs>


## Prometheus

When configured, LiveKit exposes Prometheus stats via /metrics endpoint. We export metrics related to rooms, participants, and packet transmissions.

To view the list of metrics we export, see [roomstatsreporter.go](https://github.com/livekit/livekit-server/blob/master/pkg/utils/stats/roomstatsreporter.go)

## Multi-node routing

If Redis is configured, LiveKit automatically switches to a distributed setup by using Redis for room data as well as a message bus. In this mode, each node periodically reports their stats to Redis; this enables them to be aware of the entire cluster and make routing decisions based on availability and load. We recommend this setup for a redundant deployment.

When a new room is created, the node that received this request is able to choose an available node from the cluster to host the room.

When a client establishes a signal connection to LiveKit, it creates a persistent WebSocket connection with one of the instances. That instance will then acts as a signaling bridge, proxying messages between the node where the room is hosted and the client.

In a multi-node setup, LiveKit can support a large number of concurrent rooms. However, there are limits to the number of participants in a room since, for now, a room must fit on a single node.
